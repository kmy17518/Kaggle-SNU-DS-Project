{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport torch\nprint(os.listdir('../input/snu-2021-1-ds-project-3'))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T06:12:47.509756Z","iopub.execute_input":"2021-06-10T06:12:47.510432Z","iopub.status.idle":"2021-06-10T06:12:48.554967Z","shell.execute_reply.started":"2021-06-10T06:12:47.510334Z","shell.execute_reply":"2021-06-10T06:12:48.553198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dir = '../input/snu-2021-1-ds-project-3/train'\ntest_dir = '../input/snu-2021-1-ds-project-3/test'","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:12:48.556438Z","iopub.execute_input":"2021-06-10T06:12:48.556779Z","iopub.status.idle":"2021-06-10T06:12:48.561401Z","shell.execute_reply.started":"2021-06-10T06:12:48.556742Z","shell.execute_reply":"2021-06-10T06:12:48.560105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions","metadata":{}},{"cell_type":"code","source":"def extract_age(info_file):\n    '''\n        info file(###.txt)로부터 나이 정보를 뽑아냅니다.\n    '''\n    with open(info_file, 'r') as f:\n        info = f.read()\n        for i, line in enumerate(info.split(\"\\n\")):\n            if line.startswith(\"#Age\"):\n                age = float(line.split(\": \")[1].strip())\n    return age\n\ndef extract_sex(info_file):\n    '''\n        info file(###.txt)로부터 성별 정보를 뽑아냅니다.\n    '''\n    with open(info_file, 'r') as f:\n            info = f.read()\n            for i, line in enumerate(info.split(\"\\n\")):\n                if line.startswith(\"#Sex\"):\n                    sex = line.split(\": \")[1].strip()\n    return sex\n\ndef extract_labels(info_file):\n    '''\n        info file(###.txt)로부터 label(들) 정보를 뽑아냅니다.\n    '''\n    with open(info_file, 'r') as f:\n            info = f.read()\n            for i, line in enumerate(info.split(\"\\n\")):\n                if line.startswith(\"#Dx\"):\n                    labels = line.split(\": \")[1].strip()\n                    labels = labels.split()\n    return labels\n\ndef read_files(data_directory, is_training=True):\n    '''\n        data directory(train 또는 test)로부터 모든 sample들의\n        id, age, sex, recording, labels 정보를 읽어들여\n        (id, age, sex, recording, labels)의 list를 반환합니다.\n        is_training=False일 경우엔 labels 정보를 읽어들이지 않습니다.\n    '''\n    list_id = []\n    list_age = []\n    list_sex = []\n    list_recording = []\n    list_labels = []\n    for f in os.listdir(data_directory):\n        root, extension = os.path.splitext(f)\n        if not root.startswith(\".\") and extension == \".txt\":\n            list_id.append(int(root))\n            info_file = os.path.join(data_directory, root + \".txt\")\n            recording_file = os.path.join(data_directory, root + \".npy\")\n            age = extract_age(info_file)\n            list_age.append(age)\n            sex = extract_sex(info_file)\n            list_sex.append(sex)\n            with open(recording_file, 'rb') as g:\n                recording = np.load(g)\n                list_recording.append(recording)\n            if is_training:\n                labels = extract_labels(info_file)\n                list_labels.append(labels)\n    if is_training:\n        return list(zip(list_id, list_age, list_sex, list_recording, list_labels))\n    else:\n        return list(zip(list_id, list_age, list_sex, list_recording))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:12:48.567712Z","iopub.execute_input":"2021-06-10T06:12:48.56826Z","iopub.status.idle":"2021-06-10T06:12:48.587264Z","shell.execute_reply.started":"2021-06-10T06:12:48.568222Z","shell.execute_reply":"2021-06-10T06:12:48.585729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch Custom Dataset\ntraining sample을 batch 단위로 처리할 수 있도록 torch.uitls.data.Dataset을 이용한 custom dataset을 만들어 줍니다.","metadata":{}},{"cell_type":"code","source":"class Dataset_ECG(torch.utils.data.Dataset):\n    \"\"\"\n        Build ECG dataset\n    \"\"\"\n    def __init__(self, dataset, num_classes=12):\n        \"\"\"\n            dataset을 읽어들여 id, age, sex, recording, labels를 저장한 list를 만들어 줍니다.\n        \"\"\"\n        self.sample_id = []\n        self.sample_age = []\n        self.sample_sex = []\n        self.sample_recording = []\n        self.sample_labels = []\n        self.num_samples = len(dataset)\n        \n        for idx in range(self.num_samples):\n            _id, _age, _sex, _recording, _labels = dataset[idx]\n            # model에 input으로 들어가는 data는 torch.Tensor 타입으로 변환해 줍니다.\n            age = torch.tensor(_age)\n            sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n            recording = torch.tensor(_recording)\n            labels = torch.tensor(np.zeros(num_classes))\n            for label in _labels:\n                labels[int(label)] = 1\n\n            self.sample_id.append(_id)\n            self.sample_age.append(age)\n            self.sample_sex.append(sex)\n            self.sample_recording.append(recording)\n            self.sample_labels.append(labels)\n\n        print(f'Loaded {self.num_samples} samples...')\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return {\n            \"id\": self.sample_id[idx],\n            \"age\": self.sample_age[idx],\n            \"sex\": self.sample_sex[idx],\n            \"recording\": self.sample_recording[idx],\n            \"labels\": self.sample_labels[idx]\n        }","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:12:48.588652Z","iopub.execute_input":"2021-06-10T06:12:48.588981Z","iopub.status.idle":"2021-06-10T06:12:48.606983Z","shell.execute_reply.started":"2021-06-10T06:12:48.588952Z","shell.execute_reply":"2021-06-10T06:12:48.605697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset_ECG_pytorch(torch.utils.data.Dataset):\n    \"\"\"\n        Build ECG dataset\n    \"\"\"\n    def __init__(self, list_id, list_age, list_sex, list_recording, list_labels_oh=None, num_classes=12):\n        \"\"\"\n            dataset을 읽어들여 id, age, sex, recording, labels를 저장한 list를 만들어 줍니다.\n        \"\"\"\n        self.sample_id = torch.tensor(list_id)\n        self.sample_age = torch.tensor(list_age)\n        self.sample_sex = torch.tensor(list_sex)\n        self.sample_recording = torch.tensor(list_recording)\n\n        length = len(list_id)\n        assert length==len(self.sample_id)\n        assert length==len(self.sample_age)\n        assert length==len(self.sample_sex)\n        assert length==len(self.sample_recording)\n\n        if not list_labels_oh is None:\n            self.train = True\n            self.sample_labels = torch.tensor(list_labels_oh)\n            assert length==len(self.sample_labels)\n        \n        self.num_samples = length\n        \n        print(f'Loaded {self.num_samples} samples...')\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        result = {\n            \"id\": self.sample_id[idx],\n            \"age\": self.sample_age[idx],\n            \"sex\": self.sample_sex[idx],\n            \"recording\": self.sample_recording[idx],\n        }\n        if self.train:\n            result['labels'] = self.sample_labels[idx]\n        return result","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:12:48.608709Z","iopub.execute_input":"2021-06-10T06:12:48.609172Z","iopub.status.idle":"2021-06-10T06:12:48.627848Z","shell.execute_reply.started":"2021-06-10T06:12:48.609123Z","shell.execute_reply":"2021-06-10T06:12:48.626516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch CNN model\n간단한 CNN model을 만들어 보겠습니다.","metadata":{}},{"cell_type":"code","source":"class Example_CNN_v1(torch.nn.Module):\n    def __init__(self, num_classes=12, num_leads=2):\n        super(Example_CNN_v1, self).__init__()\n        self.num_classes = num_classes\n        self.num_leads = num_leads\n        self.conv1 = torch.nn.Conv1d(in_channels=self.num_leads, out_channels=32, kernel_size=15, stride=3, padding=2)\n        self.relu1 = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv1d(in_channels=32, out_channels=64, kernel_size=13, stride=3, padding=1)\n        self.relu2 = torch.nn.ReLU()\n        self.conv3 = torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=10, stride=2)\n        self.relu3 = torch.nn.ReLU()\n        self.conv4 = torch.nn.Conv1d(in_channels=128, out_channels=64, kernel_size=8, stride=2)\n        self.relu4 = torch.nn.ReLU()\n        self.conv5 = torch.nn.Conv1d(in_channels=64, out_channels=32, kernel_size=7, stride=2)\n        self.relu5 = torch.nn.ReLU()\n        self.fc1 = torch.nn.Linear(32*64, 128)\n        self.relu6 = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(128, self.num_classes)\n\n    def forward(self, x):\n        # 이 모델은 recording만을 input으로 받습니다. feature를 추가적으로 사용하도록 할 수도 있습니다.\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.conv4(x)\n        x = self.relu4(x)\n        x = self.conv5(x)\n        x = self.relu5(x)\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = self.fc1(x)\n        x = self.relu6(x)\n        out = self.fc2(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:12:48.629512Z","iopub.execute_input":"2021-06-10T06:12:48.629909Z","iopub.status.idle":"2021-06-10T06:12:48.647116Z","shell.execute_reply.started":"2021-06-10T06:12:48.629874Z","shell.execute_reply":"2021-06-10T06:12:48.645446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"code","source":"import pickle\n\nbase_dir = '../input/preprocessed/'\nwith open(base_dir+'test_data_torch.pkl', 'rb') as f:\n    testing_dataset = pickle.load(f)\nwith open(base_dir+'train_data_torch.pkl', 'rb') as f:\n    training_dataset = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:12:48.649858Z","iopub.execute_input":"2021-06-10T06:12:48.650322Z","iopub.status.idle":"2021-06-10T06:13:08.493099Z","shell.execute_reply.started":"2021-06-10T06:12:48.650275Z","shell.execute_reply":"2021-06-10T06:13:08.492104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataset.sample_recording","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:08.494672Z","iopub.execute_input":"2021-06-10T06:13:08.494956Z","iopub.status.idle":"2021-06-10T06:13:08.566825Z","shell.execute_reply.started":"2021-06-10T06:13:08.494928Z","shell.execute_reply":"2021-06-10T06:13:08.565804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cuda gpu를 사용할 수 있을 경우 사용합니다.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:08.5681Z","iopub.execute_input":"2021-06-10T06:13:08.568681Z","iopub.status.idle":"2021-06-10T06:13:08.575138Z","shell.execute_reply.started":"2021-06-10T06:13:08.568635Z","shell.execute_reply":"2021-06-10T06:13:08.573948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(training_dataset.sample_recording)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:08.576432Z","iopub.execute_input":"2021-06-10T06:13:08.576744Z","iopub.status.idle":"2021-06-10T06:13:08.590878Z","shell.execute_reply.started":"2021-06-10T06:13:08.576716Z","shell.execute_reply":"2021-06-10T06:13:08.589919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(training_dataset.sample_recording[0][0])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:08.592277Z","iopub.execute_input":"2021-06-10T06:13:08.59295Z","iopub.status.idle":"2021-06-10T06:13:08.605476Z","shell.execute_reply.started":"2021-06-10T06:13:08.592904Z","shell.execute_reply":"2021-06-10T06:13:08.604613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (18, 5)\ntest_graph = training_dataset.sample_recording[0][1]\nplt.plot(test_graph)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:34:28.062711Z","iopub.execute_input":"2021-06-10T07:34:28.063091Z","iopub.status.idle":"2021-06-10T07:34:28.251791Z","shell.execute_reply.started":"2021-06-10T07:34:28.063049Z","shell.execute_reply":"2021-06-10T07:34:28.250546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks\n\npeaks, properties = find_peaks(test_graph, height = 0, width =0, distance=200, prominence = 0.25)\n\nprint(f'Index of each peak : {peaks}')\nprint(f'Height of each peak : {properties[\"peak_heights\"]}')\n\n\nplt.plot(test_graph)\nplt.plot(peaks, test_graph[peaks], \"x\")\n      \n\n# height = peaks[1]['peak_heights'] #list of the heights of the peaks\n# peak_pos = x[peaks[0]] #list of the peaks positions","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:34:33.271512Z","iopub.execute_input":"2021-06-10T07:34:33.271869Z","iopub.status.idle":"2021-06-10T07:34:33.465121Z","shell.execute_reply.started":"2021-06-10T07:34:33.271839Z","shell.execute_reply":"2021-06-10T07:34:33.46405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rr_interval = []\nfor j in range(len(peaks)-1):\n    rr = (peaks[j+1]-peaks[j-1])\n    rr_interval.append(rr)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:09.865566Z","iopub.execute_input":"2021-06-10T06:13:09.865877Z","iopub.status.idle":"2021-06-10T06:13:09.870427Z","shell.execute_reply.started":"2021-06-10T06:13:09.865849Z","shell.execute_reply":"2021-06-10T06:13:09.869441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = sum(rr_interval)/len(rr_interval)\nprint(mean)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:09.872387Z","iopub.execute_input":"2021-06-10T06:13:09.87268Z","iopub.status.idle":"2021-06-10T06:13:09.886964Z","shell.execute_reply.started":"2021-06-10T06:13:09.872652Z","shell.execute_reply":"2021-06-10T06:13:09.885859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n!pip install PyWavelets\nimport pywt as pw\n\ndef QRS_detection(signal,sample_rate,max_bpm):\n\n    ## Stationary Wavelet Transform\n    coeffs = pw.swt(signal, wavelet = \"haar\", level=2, start_level=0, axis=-1)\n    d2 = coeffs[1][1] ##2nd level detail coefficients\n    \n    \n    ## Threhold the detail coefficients\n    avg = np.mean(d2)\n    std = np.std(d2)\n    sig_thres = [abs(i) if abs(i)>2.0*std else 0 for i in d2-avg]\n    \n    ## Find the maximum modulus in each window\n    window = int((60.0/max_bpm)*sample_rate)\n    sig_len = len(signal)\n    n_windows = int(sig_len/window)\n    modulus,qrs = [],[]\n    \n    ##Loop through windows and find max modulus\n    for i in range(n_windows):\n        start = i*window\n        end = min([(i+1)*window,sig_len])\n        mx = max(sig_thres[start:end])\n        if mx>0:\n            modulus.append( (start + np.argmax(sig_thres[start:end]),mx))\n    \n    \n    ## Merge if within max bpm\n    merge_width = int((0.2)*sample_rate)\n    i=0\n    while i < len(modulus)-1:\n        ann = modulus[i][0]\n        if modulus[i+1][0]-modulus[i][0] < merge_width:\n            if modulus[i+1][1]>modulus[i][1]: # Take larger modulus\n                ann = modulus[i+1][0]\n            i+=1\n                \n        qrs.append(ann)\n        i+=1 \n    ## Pin point exact qrs peak\n    window_check = int(sample_rate/6)\n    #signal_normed = np.absolute((signal-np.mean(signal))/(max(signal)-min(signal)))\n    r_peaks = [0]*len(qrs)\n    \n    for i,loc in enumerate(qrs):\n        start = max(0,loc-window_check)\n        end = min(sig_len,loc+window_check)\n        wdw = np.absolute(signal[start:end] - np.mean(signal[start:end]))\n        pk = np.argmax(wdw)\n        r_peaks[i] = start+pk\n        \n    return r_peaks","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:09.888501Z","iopub.execute_input":"2021-06-10T06:13:09.8888Z","iopub.status.idle":"2021-06-10T06:13:18.124573Z","shell.execute_reply.started":"2021-06-10T06:13:09.888772Z","shell.execute_reply":"2021-06-10T06:13:18.123124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (18, 5)\ntest_graph = training_dataset.sample_recording[507][1]\nplt.plot(test_graph)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:28:38.418954Z","iopub.execute_input":"2021-06-10T07:28:38.419448Z","iopub.status.idle":"2021-06-10T07:28:38.615096Z","shell.execute_reply.started":"2021-06-10T07:28:38.419411Z","shell.execute_reply":"2021-06-10T07:28:38.614186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing = test_graph.numpy()\npeaks = QRS_detection(testing, 600, 220)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.314152Z","iopub.execute_input":"2021-06-10T06:13:18.314596Z","iopub.status.idle":"2021-06-10T06:13:18.326307Z","shell.execute_reply.started":"2021-06-10T06:13:18.31454Z","shell.execute_reply":"2021-06-10T06:13:18.325231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_graph)\nplt.plot(peaks, test_graph[peaks], \"x\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.327599Z","iopub.execute_input":"2021-06-10T06:13:18.327972Z","iopub.status.idle":"2021-06-10T06:13:18.509349Z","shell.execute_reply.started":"2021-06-10T06:13:18.327941Z","shell.execute_reply":"2021-06-10T06:13:18.508133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outliers_iqr(data):\n    q1, q3 = np.percentile(data, [25,75])\n    \n    iqr = q3 - q1\n    lower_bound = q1/2.5\n    upper_bound = q3*2.5 \n    \n    return np.where((data > upper_bound) | (data < lower_bound))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.510564Z","iopub.execute_input":"2021-06-10T06:13:18.510853Z","iopub.status.idle":"2021-06-10T06:13:18.518287Z","shell.execute_reply.started":"2021-06-10T06:13:18.510825Z","shell.execute_reply":"2021-06-10T06:13:18.516886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = test_graph[peaks]\noutliers = outliers_iqr(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.519772Z","iopub.execute_input":"2021-06-10T06:13:18.520137Z","iopub.status.idle":"2021-06-10T06:13:18.533015Z","shell.execute_reply.started":"2021-06-10T06:13:18.520089Z","shell.execute_reply":"2021-06-10T06:13:18.532143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.534298Z","iopub.execute_input":"2021-06-10T06:13:18.534752Z","iopub.status.idle":"2021-06-10T06:13:18.544954Z","shell.execute_reply.started":"2021-06-10T06:13:18.53472Z","shell.execute_reply":"2021-06-10T06:13:18.54376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_peak(graph, peaks):\n    diff_graph_peaks = np.diff(graph[peaks], n=1, axis=-1)\n    to_exclude = []\n    # 기울기 검사\n    for i in range(len(diff_graph_peaks)):\n        if diff_graph_peaks[i]>1 or diff_graph_peaks[i]<-1:\n            to_exclude.append(i)\n    peaks = np.delete(peaks, to_exclude)\n    # 위로볼록/아래로 볼록\n    diff_graph =np.diff(graph, n=1, axis=-1)\n    validated_peaks = []\n    for i in range(len(peaks)):\n        if diff_graph[peaks[i]]>0:\n            for j in range(10):\n                if diff_graph[peaks[i]+j]<0:\n                    validated_peaks.append(peaks[i])\n                    break\n        elif diff_graph[peaks[i]]<0:\n            for j in range(10):\n                if diff_graph[peaks[i]-j]>0:\n                    validated_peaks.append(peaks[i])\n                    break\n        else:\n            flag = False\n            for j in range(10):\n                if diff_graph[peaks[i]+j]<0:\n                    flag = True\n                    break\n            if flag == True:\n                for k in range(10):\n                    if diff_graph[peaks[i]-k]>0:\n                        validated_peaks.append(peaks[i])\n                        break\n    return validated_peaks\n\n                            \n                    \n                \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.546519Z","iopub.execute_input":"2021-06-10T06:13:18.546983Z","iopub.status.idle":"2021-06-10T06:13:18.560566Z","shell.execute_reply.started":"2021-06-10T06:13:18.546949Z","shell.execute_reply":"2021-06-10T06:13:18.559246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_peak_2(graph, peaks):\n    general_peaks, properties = find_peaks(graph, height = 0, width =0)\n    validated_peaks = []\n    for val in peaks:\n        if val in general_peaks:\n            validated_peaks.append(val)\n    return validated_peaks\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.562168Z","iopub.execute_input":"2021-06-10T06:13:18.562697Z","iopub.status.idle":"2021-06-10T06:13:18.575122Z","shell.execute_reply.started":"2021-06-10T06:13:18.562642Z","shell.execute_reply":"2021-06-10T06:13:18.573986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validated_peaks = is_peak_2(test_graph, peaks)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.576439Z","iopub.execute_input":"2021-06-10T06:13:18.577004Z","iopub.status.idle":"2021-06-10T06:13:18.588769Z","shell.execute_reply.started":"2021-06-10T06:13:18.576955Z","shell.execute_reply":"2021-06-10T06:13:18.587982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validated_peaks","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.590112Z","iopub.execute_input":"2021-06-10T06:13:18.590577Z","iopub.status.idle":"2021-06-10T06:13:18.603432Z","shell.execute_reply.started":"2021-06-10T06:13:18.590532Z","shell.execute_reply":"2021-06-10T06:13:18.602349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_graph)\nplt.plot(validated_peaks, test_graph[validated_peaks], \"x\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.604993Z","iopub.execute_input":"2021-06-10T06:13:18.605452Z","iopub.status.idle":"2021-06-10T06:13:18.786259Z","shell.execute_reply.started":"2021-06-10T06:13:18.605418Z","shell.execute_reply":"2021-06-10T06:13:18.78522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graph = training_dataset.sample_recording[0][1]\npeaks, properties = find_peaks(graph, height = 0, width =0, distance=150, prominence = 0.25)\nrr_interval = []\nfor j in range(len(peaks)-1):\n    rr = (peaks[j+1]-peaks[j-1])*0.002\n    rr_interval.append(rr)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.787611Z","iopub.execute_input":"2021-06-10T06:13:18.787902Z","iopub.status.idle":"2021-06-10T06:13:18.79461Z","shell.execute_reply.started":"2021-06-10T06:13:18.787873Z","shell.execute_reply":"2021-06-10T06:13:18.793672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peaks","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.796074Z","iopub.execute_input":"2021-06-10T06:13:18.796382Z","iopub.status.idle":"2021-06-10T06:13:18.810436Z","shell.execute_reply.started":"2021-06-10T06:13:18.796351Z","shell.execute_reply":"2021-06-10T06:13:18.809171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks\n\nrr_intervals = []\n\nfor i in range(len(training_dataset.sample_recording)):\n    graph = training_dataset.sample_recording[i][1]\n    peaks, properties = find_peaks(graph, height = 0, width =0, distance=200, prominence = 0.25)\n    rr_interval = []\n    for j in range(len(peaks)-1):\n        rr_interval.append((peaks[j+1]-peaks[j])*0.002)\n    rr_intervals.append(rr_interval)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:18.811894Z","iopub.execute_input":"2021-06-10T06:13:18.812254Z","iopub.status.idle":"2021-06-10T06:13:25.079604Z","shell.execute_reply.started":"2021-06-10T06:13:18.812223Z","shell.execute_reply":"2021-06-10T06:13:25.078407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 선택적! \nfor i in range(len(rr_intervals)):\n    if len(rr_intervals[i]) ==0:\n        testing = training_dataset.sample_recording[i][1].numpy()\n        candidate_peaks = QRS_detection(testing, 600, 220)\n        validated_peaks = is_peak_2(testing, candidate_peaks)\n        rr_interval2 = []\n        for j in range(len(validated_peaks)-1):\n            rr_interval2.append((validated_peaks[j+1]-validated_peaks[j])*0.002)\n        rr_intervals[i] = rr_interval2\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:25.081099Z","iopub.execute_input":"2021-06-10T06:13:25.081414Z","iopub.status.idle":"2021-06-10T06:13:25.843506Z","shell.execute_reply.started":"2021-06-10T06:13:25.081385Z","shell.execute_reply":"2021-06-10T06:13:25.842256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rr_interval outlier 제거 -> 선택적! \nfor i in range(len(rr_intervals)):\n    if len(rr_intervals[i]) !=0:\n        outliers = outliers_iqr(rr_intervals[i])\n        rr_intervals[i] = np.delete(rr_intervals[i], outliers)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:25.844653Z","iopub.execute_input":"2021-06-10T06:13:25.844953Z","iopub.status.idle":"2021-06-10T06:13:29.184163Z","shell.execute_reply.started":"2021-06-10T06:13:25.844923Z","shell.execute_reply":"2021-06-10T06:13:29.183131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hr_min\nhr_min = []\nfor i in range(len(rr_intervals)):\n    if len(rr_intervals[i]) != 0:\n        max_int = max(rr_intervals[i])\n        val = 60/max_int\n        hr_min.append(val)\n    else:\n        hr_min.append(10000)\n    \n# hr_max\nhr_max = []\nfor i in range(len(rr_intervals)):\n    if len(rr_intervals[i]) != 0:\n        val = 60/min(rr_intervals[i])\n        hr_max.append(val)\n    else:\n        hr_max.append(10000)\n\n# rr_interval_median\nrr_interval_median = []\nfor i in range(len(rr_intervals)):\n    if len(rr_intervals[i]) != 0:\n        sorted_intervals = sorted(rr_intervals[i])\n        val = sorted_intervals[len(sorted_intervals)//2]\n        rr_interval_median.append(val)\n    else:\n        rr_interval_median.append(10000)\n\n# hr_mean\nhr_mean = []\nfor i in range(len(rr_intervals)):\n    if len(rr_intervals[i]) != 0:\n        sum_hr = 0\n        for val in rr_intervals[i]:\n            sum_hr+= 60/(val)\n        value = sum_hr/len(rr_intervals[i])\n        hr_mean.append(value)\n    else:\n        hr_mean.append(10000)\n    \n# print(len(hr_min))\n# # r peak 못찾은 값 평균값 대체\n# cnt_rpeak_zeros = 0\n# for i in range(len(rr_intervals)):\n#     if hr_min[i] ==-1:\n#         hr_min[i] = np.mean(hr_min)\n#         cnt_rpeak_zeros+=1\n#     if hr_max[i] ==-1:\n#         hr_max[i] = np.mean(hr_max)\n#     if hr_mean[i] == -1:\n#         hr_mean[i] = np.mean(hr_mean)\n#     if rr_interval_median[i] == -1:\n#         rr_interval_median[i] = np.mean(rr_interval_median)\n\n# print(cnt_rpeak_zeros)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:29.18577Z","iopub.execute_input":"2021-06-10T06:13:29.18621Z","iopub.status.idle":"2021-06-10T06:13:29.754377Z","shell.execute_reply.started":"2021-06-10T06:13:29.186164Z","shell.execute_reply":"2021-06-10T06:13:29.753362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ncnt_rpeak_ones = 0\n# diff_rr_min\ndiff_rr_min = []\nfor i in range(len(rr_intervals)):\n    if len(rr_intervals[i]) != 0:\n        diff_rr = np.diff(rr_intervals[i], n=1, axis=-1)\n        if len(diff_rr) !=0:\n            diff_rr_min.append(min(diff_rr))\n        else:\n            diff_rr_min.append(10000)\n            cnt_rpeak_ones+=1\n    else:\n        diff_rr_min.append(10000) \n    \n\nprint(cnt_rpeak_ones)\n\n# # r peak 못찾은 값 or r peak 1개 -> 평균값 대체\n# for i in range(len(rr_intervals)):\n#     if diff_rr_min[i] ==100:\n#         diff_rr_min[i] = np.mean(diff_rr_min)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:29.75963Z","iopub.execute_input":"2021-06-10T06:13:29.75998Z","iopub.status.idle":"2021-06-10T06:13:29.991917Z","shell.execute_reply.started":"2021-06-10T06:13:29.759945Z","shell.execute_reply":"2021-06-10T06:13:29.990599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(hr_min, columns = ['hr_min'])\ndf['hr_max'] = hr_max\ndf['rr_interval_median'] = rr_interval_median\ndf['hr_mean'] = hr_mean\ndf['diff_rr_min'] = diff_rr_min\ndf['id'] = training_dataset.sample_id","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:29.994421Z","iopub.execute_input":"2021-06-10T06:13:29.994765Z","iopub.status.idle":"2021-06-10T06:13:30.093431Z","shell.execute_reply.started":"2021-06-10T06:13:29.99473Z","shell.execute_reply":"2021-06-10T06:13:30.091911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.095147Z","iopub.execute_input":"2021-06-10T06:13:30.09579Z","iopub.status.idle":"2021-06-10T06:13:30.133578Z","shell.execute_reply.started":"2021-06-10T06:13:30.095671Z","shell.execute_reply":"2021-06-10T06:13:30.132355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('feature_extraction_train_data1.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.134906Z","iopub.execute_input":"2021-06-10T06:13:30.135263Z","iopub.status.idle":"2021-06-10T06:13:30.347347Z","shell.execute_reply.started":"2021-06-10T06:13:30.135231Z","shell.execute_reply":"2021-06-10T06:13:30.346283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass real_time_peak_detection():\n    def __init__(self, array, lag, threshold, influence):\n        self.y = list(array)\n        self.length = len(self.y)\n        self.lag = lag\n        self.threshold = threshold\n        self.influence = influence\n        self.signals = [0] * len(self.y)\n        self.filteredY = np.array(self.y).tolist()\n        self.avgFilter = [0] * len(self.y)\n        self.stdFilter = [0] * len(self.y)\n        self.avgFilter[self.lag - 1] = np.mean(self.y[0:self.lag]).tolist()\n        self.stdFilter[self.lag - 1] = np.std(self.y[0:self.lag]).tolist()\n\n    def thresholding_algo(self, new_value):\n        self.y.append(new_value)\n        i = len(self.y) - 1\n        self.length = len(self.y)\n        if i < self.lag:\n            return 0\n        elif i == self.lag:\n            self.signals = [0] * len(self.y)\n            self.filteredY = np.array(self.y).tolist()\n            self.avgFilter = [0] * len(self.y)\n            self.stdFilter = [0] * len(self.y)\n            self.avgFilter[self.lag - 1] = np.mean(self.y[0:self.lag]).tolist()\n            self.stdFilter[self.lag - 1] = np.std(self.y[0:self.lag]).tolist()\n            return 0\n\n        self.signals += [0]\n        self.filteredY += [0]\n        self.avgFilter += [0]\n        self.stdFilter += [0]\n\n        if abs(self.y[i] - self.avgFilter[i - 1]) > self.threshold * self.stdFilter[i - 1]:\n            if self.y[i] > self.avgFilter[i - 1]:\n                self.signals[i] = 1\n            else:\n                self.signals[i] = -1\n\n            self.filteredY[i] = self.influence * self.y[i] + (1 - self.influence) * self.filteredY[i - 1]\n            self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n            self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n        else:\n            self.signals[i] = 0\n            self.filteredY[i] = self.y[i]\n            self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n            self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n        print(i)\n        return self.signals[i]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.348886Z","iopub.execute_input":"2021-06-10T06:13:30.349309Z","iopub.status.idle":"2021-06-10T06:13:30.36809Z","shell.execute_reply.started":"2021-06-10T06:13:30.349266Z","shell.execute_reply":"2021-06-10T06:13:30.367089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signals = real_time_peak_detection(test_graph,5, 3.5, 0.5)\nsignals = signals.thresholding_algo(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.369392Z","iopub.execute_input":"2021-06-10T06:13:30.369742Z","iopub.status.idle":"2021-06-10T06:13:30.421519Z","shell.execute_reply.started":"2021-06-10T06:13:30.369711Z","shell.execute_reply":"2021-06-10T06:13:30.420414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python\n# Implementation of algorithm from https://stackoverflow.com/a/22640362/6029703\nimport numpy as np\nimport pylab\n\ndef thresholding_algo(y, lag, threshold, influence):\n    signals = np.zeros(len(y))\n    filteredY = np.array(y)\n    avgFilter = [0]*len(y)\n    stdFilter = [0]*len(y)\n    avgFilter[lag - 1] = np.mean(y[0:lag])\n    stdFilter[lag - 1] = np.std(y[0:lag])\n    for i in range(lag, len(y)):\n        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:\n            if y[i] > avgFilter[i-1]:\n                signals[i] = 1\n            else:\n                signals[i] = -1\n\n            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]\n            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n        else:\n            signals[i] = 0\n            filteredY[i] = y[i]\n            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n\n    return dict(signals = np.asarray(signals),\n                avgFilter = np.asarray(avgFilter),\n                stdFilter = np.asarray(stdFilter))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.422877Z","iopub.execute_input":"2021-06-10T06:13:30.423219Z","iopub.status.idle":"2021-06-10T06:13:30.439235Z","shell.execute_reply.started":"2021-06-10T06:13:30.423186Z","shell.execute_reply":"2021-06-10T06:13:30.438208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data\ny = test_graph.numpy()\n\n# Settings: lag = 30, threshold = 5, influence = 0\nlag = 30\nthreshold = 5\ninfluence = 0\n\n# Run algo with settings from above\nresult = thresholding_algo(y, lag=lag, threshold=threshold, influence=influence)\n\n# Plot result\npylab.subplot(211)\npylab.plot(np.arange(1, len(y)+1), y)\n\npylab.plot(np.arange(1, len(y)+1),\n           result[\"avgFilter\"], color=\"cyan\", lw=2)\n\npylab.plot(np.arange(1, len(y)+1),\n           result[\"avgFilter\"] + threshold * result[\"stdFilter\"], color=\"green\", lw=2)\n\npylab.plot(np.arange(1, len(y)+1),\n           result[\"avgFilter\"] - threshold * result[\"stdFilter\"], color=\"green\", lw=2)\n\npylab.subplot(212)\npylab.step(np.arange(1, len(y)+1), result[\"signals\"], color=\"red\", lw=2)\npylab.ylim(-1.5, 1.5)\npylab.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.440935Z","iopub.execute_input":"2021-06-10T06:13:30.441701Z","iopub.status.idle":"2021-06-10T06:13:30.910645Z","shell.execute_reply.started":"2021-06-10T06:13:30.44165Z","shell.execute_reply":"2021-06-10T06:13:30.909593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signals","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.912Z","iopub.execute_input":"2021-06-10T06:13:30.91234Z","iopub.status.idle":"2021-06-10T06:13:30.918287Z","shell.execute_reply.started":"2021-06-10T06:13:30.912309Z","shell.execute_reply":"2021-06-10T06:13:30.917345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topid= sorted(range(len(test_graph)),key= lambda i: test_graph[i])[-100:]\nprint(topid)\ntest_graph[topid]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:30.919668Z","iopub.execute_input":"2021-06-10T06:13:30.920063Z","iopub.status.idle":"2021-06-10T06:13:31.502044Z","shell.execute_reply.started":"2021-06-10T06:13:30.920008Z","shell.execute_reply":"2021-06-10T06:13:31.50104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ntest_diff = np.diff(test_graph, n=1, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:31.503335Z","iopub.execute_input":"2021-06-10T06:13:31.503654Z","iopub.status.idle":"2021-06-10T06:13:31.508707Z","shell.execute_reply.started":"2021-06-10T06:13:31.503624Z","shell.execute_reply":"2021-06-10T06:13:31.507683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_diff)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:31.509961Z","iopub.execute_input":"2021-06-10T06:13:31.510339Z","iopub.status.idle":"2021-06-10T06:13:31.670269Z","shell.execute_reply.started":"2021-06-10T06:13:31.510301Z","shell.execute_reply":"2021-06-10T06:13:31.669272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MIN_HR\nmin_hr = []\nfor i in range(len(training_dataset.sample_recording)):\n    arr = []\n    min_hr_1 = min(training_dataset.sample_recording[i][0])\n    min_hr_2 = min(training_dataset.sample_recording[i][1])\n    arr.append(min_hr_1)\n    arr.append(min_hr_2)\n    min_hr.append\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:13:31.671595Z","iopub.execute_input":"2021-06-10T06:13:31.67194Z","iopub.status.idle":"2021-06-10T06:47:46.496445Z","shell.execute_reply.started":"2021-06-10T06:13:31.671906Z","shell.execute_reply":"2021-06-10T06:47:46.495227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_HR\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.49807Z","iopub.execute_input":"2021-06-10T06:47:46.498363Z","iopub.status.idle":"2021-06-10T06:47:46.501404Z","shell.execute_reply.started":"2021-06-10T06:47:46.498334Z","shell.execute_reply":"2021-06-10T06:47:46.500681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training에 사용될 hyperparameter를 정해줍니다.\nEPOCHS = 15\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.502659Z","iopub.execute_input":"2021-06-10T06:47:46.503168Z","iopub.status.idle":"2021-06-10T06:47:46.519827Z","shell.execute_reply.started":"2021-06-10T06:47:46.503115Z","shell.execute_reply":"2021-06-10T06:47:46.518946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training dataset을 batch 단위로 읽어들일 수 있도록 DataLoader를 만들어줍니다.\ntraining_loader = torch.utils.data.DataLoader(training_dataset, pin_memory=True, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.521368Z","iopub.execute_input":"2021-06-10T06:47:46.521678Z","iopub.status.idle":"2021-06-10T06:47:46.537723Z","shell.execute_reply.started":"2021-06-10T06:47:46.521646Z","shell.execute_reply":"2021-06-10T06:47:46.536365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"model = Example_CNN_v1(num_classes=12, num_leads=2)\n\nmodel.to(device)\nmodel.train()\n\ncriterion = torch.nn.BCEWithLogitsLoss() # for multi-label classification\noptimizer = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.539338Z","iopub.execute_input":"2021-06-10T06:47:46.539821Z","iopub.status.idle":"2021-06-10T06:47:46.575557Z","shell.execute_reply.started":"2021-06-10T06:47:46.539777Z","shell.execute_reply":"2021-06-10T06:47:46.574634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(1, EPOCHS+1):\n    print(f'***** Epoch {epoch} *****')\n    epoch_training_loss_sum = 0.0\n    for i_batch, sample_batched in enumerate(training_loader):\n        b_recording = sample_batched[\"recording\"].to(device)\n        b_labels = sample_batched[\"labels\"].to(device)\n        optimizer.zero_grad()\n        b_out = model(b_recording)\n        loss = criterion(b_out, b_labels)\n        loss.backward()\n        optimizer.step()\n        epoch_training_loss_sum += loss.item() * b_labels.shape[0]\n\n    epoch_training_loss = epoch_training_loss_sum / num_training\n    print(f'training loss of epoch {epoch}: {epoch_training_loss}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.576755Z","iopub.execute_input":"2021-06-10T06:47:46.577067Z","iopub.status.idle":"2021-06-10T06:47:46.781723Z","shell.execute_reply.started":"2021-06-10T06:47:46.577007Z","shell.execute_reply":"2021-06-10T06:47:46.779768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation\nevalutate on validation set","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nvalidation_prediction_df = pd.DataFrame(columns=['labels'])\nvalidation_prediction_df.index.name = 'id'\nvalidation_true_labels_df = pd.DataFrame(columns=['labels'])\nvalidation_true_labels_df.index.name = 'id'\n\nwith torch.no_grad():\n    for idx in range(len(validation_set)):\n        validation_sample = validation_set[idx]\n        _, _, _, recording, labels = validation_sample\n        out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n        sample_prediction = torch.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n        indices_of_1s = np.where(sample_prediction.cpu())[0]\n        str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n        validation_prediction_df.loc[idx] = [str_indices_of_1s]\n        \n        str_true_labels = ' '.join(labels)\n        validation_true_labels_df.loc[idx] = [str_true_labels]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.782865Z","iopub.status.idle":"2021-06-10T06:47:46.783519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(validation_prediction_df[:10])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.784607Z","iopub.status.idle":"2021-06-10T06:47:46.785228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(validation_true_labels_df[:10])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.786138Z","iopub.status.idle":"2021-06-10T06:47:46.786706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import f1_score\n\nmlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\nmlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n\nmacro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\nprint(f'macro f1 score on validation set: {macro_f1_validation}')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.787608Z","iopub.status.idle":"2021-06-10T06:47:46.788263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Prediction\n학습된 모델로 test_set에 대한 prediction을 진행합니다.","metadata":{}},{"cell_type":"code","source":"test_set = sorted(read_files(test_dir, is_training=False), key=lambda sample:sample[0])\nnum_test = len(test_set)\nprint(f'Number of test samples: {num_test}')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.789226Z","iopub.status.idle":"2021-06-10T06:47:46.789878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ntest_prediction_df = pd.DataFrame(columns=['labels'])\ntest_prediction_df.index.name = 'id'\n\nwith torch.no_grad():\n    for idx in range(len(test_set)):\n        test_sample = test_set[idx]\n        _, _, _, recording = test_sample\n        out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n        sample_prediction = torch.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n        indices_of_1s = np.where(sample_prediction.cpu())[0]\n        str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n        test_prediction_df.loc[idx] = [str_indices_of_1s]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.791021Z","iopub.status.idle":"2021-06-10T06:47:46.791531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prediction_df[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.792602Z","iopub.status.idle":"2021-06-10T06:47:46.793094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prediction_df.to_csv('my_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T06:47:46.794062Z","iopub.status.idle":"2021-06-10T06:47:46.794514Z"},"trusted":true},"execution_count":null,"outputs":[]}]}