{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n \nimport os\nimport torch\nprint(os.listdir('../input/snu-2021-1-ds-project-3'))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"E70o7ieKraBZ","executionInfo":{"status":"ok","timestamp":1622174869910,"user_tz":-540,"elapsed":18416,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"outputId":"85217a5d-90df-45e1-84a7-14aeb299697e","execution":{"iopub.status.busy":"2021-06-03T20:53:29.233931Z","iopub.execute_input":"2021-06-03T20:53:29.234346Z","iopub.status.idle":"2021-06-03T20:53:29.550608Z","shell.execute_reply.started":"2021-06-03T20:53:29.234262Z","shell.execute_reply":"2021-06-03T20:53:29.549613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dir = '../input/snu-2021-1-ds-project-3/train'\ntest_dir = '../input/snu-2021-1-ds-project-3/test'","metadata":{"id":"h309rlw1rrbd","executionInfo":{"status":"ok","timestamp":1622174873636,"user_tz":-540,"elapsed":3730,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-03T20:53:29.55242Z","iopub.execute_input":"2021-06-03T20:53:29.552664Z","iopub.status.idle":"2021-06-03T20:53:29.559276Z","shell.execute_reply.started":"2021-06-03T20:53:29.552639Z","shell.execute_reply":"2021-06-03T20:53:29.55848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_age(info_file):\n    '''\n        info file(###.txt)로부터 나이 정보를 뽑아냅니다.\n    '''\n    with open(info_file, 'r') as f:\n        info = f.read()\n        for i, line in enumerate(info.split(\"\\n\")):\n            if line.startswith(\"#Age\"):\n                age = float(line.split(\": \")[1].strip())\n    return age\n\ndef extract_sex(info_file):\n    '''\n        info file(###.txt)로부터 성별 정보를 뽑아냅니다.\n    '''\n    with open(info_file, 'r') as f:\n            info = f.read()\n            for i, line in enumerate(info.split(\"\\n\")):\n                if line.startswith(\"#Sex\"):\n                    sex = line.split(\": \")[1].strip()\n    return sex\n\ndef extract_labels(info_file):\n    '''\n        info file(###.txt)로부터 label(들) 정보를 뽑아냅니다.\n    '''\n    with open(info_file, 'r') as f:\n            info = f.read()\n            for i, line in enumerate(info.split(\"\\n\")):\n                if line.startswith(\"#Dx\"):\n                    labels = line.split(\": \")[1].strip()\n                    labels = labels.split()\n    return labels\n\ndef read_files(data_directory, is_training=True):\n    '''\n        data directory(train 또는 test)로부터 모든 sample들의\n        id, age, sex, recording, labels 정보를 읽어들여\n        (id, age, sex, recording, labels)의 list를 반환합니다.\n        is_training=False일 경우엔 labels 정보를 읽어들이지 않습니다.\n    '''\n    list_id = []\n    list_age = []\n    list_sex = []\n    list_recording = []\n    list_labels = []\n    for f in os.listdir(data_directory):\n        root, extension = os.path.splitext(f)\n        if not root.startswith(\".\") and extension == \".txt\":\n            list_id.append(int(root))\n            info_file = os.path.join(data_directory, root + \".txt\")\n            recording_file = os.path.join(data_directory, root + \".npy\")\n            age = extract_age(info_file)\n            list_age.append(age)\n            sex = extract_sex(info_file)\n            list_sex.append(sex)\n            with open(recording_file, 'rb') as g:\n                recording = np.load(g)\n                list_recording.append(recording)\n            if is_training:\n                labels = extract_labels(info_file)\n                list_labels.append(labels)\n    if is_training:\n        return list(zip(list_id, list_age, list_sex, list_recording, list_labels))\n    else:\n        return list(zip(list_id, list_age, list_sex, list_recording))","metadata":{"id":"_r9iZVZwryid","executionInfo":{"status":"ok","timestamp":1622174905771,"user_tz":-540,"elapsed":16,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-03T20:53:29.562354Z","iopub.execute_input":"2021-06-03T20:53:29.562609Z","iopub.status.idle":"2021-06-03T20:53:29.578378Z","shell.execute_reply.started":"2021-06-03T20:53:29.562578Z","shell.execute_reply":"2021-06-03T20:53:29.577565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_training_set = sorted(read_files(training_dir), key=lambda sample: sample[0])\ntotal_num_training = len(total_training_set)\nprint(f\"Number of total training samples: {total_num_training}\")\n\nnum_validation = int(total_num_training * 0.2)\nnum_training = total_num_training - num_validation\n\nprint(f'Number of validation samples: {num_validation}')\nprint(f'Number of training samples: {num_training}')","metadata":{"id":"YKKDe1mIrwmj","executionInfo":{"status":"ok","timestamp":1622174896778,"user_tz":-540,"elapsed":23145,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-03T20:53:29.579767Z","iopub.execute_input":"2021-06-03T20:53:29.580232Z","iopub.status.idle":"2021-06-03T20:54:09.749166Z","shell.execute_reply.started":"2021-06-03T20:53:29.580195Z","shell.execute_reply":"2021-06-03T20:54:09.748292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new_features = pd.read_csv('../input/feature-extraction-3/feature_extraction_train_data1.csv')\n# new_features.drop(['Unnamed: 0', 'id'] ,axis=1, inplace=True)\n\n# original_features = pd.DataFrame(total_training_set)\n# original_features.columns = ['id', 'age', 'sex', 'recording', 'label']\n\n# final_features = pd.concat([original_features, new_features], axis=1)\n# final_features = final_features.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:54:09.750466Z","iopub.execute_input":"2021-06-03T20:54:09.750985Z","iopub.status.idle":"2021-06-03T20:54:09.754635Z","shell.execute_reply.started":"2021-06-03T20:54:09.750946Z","shell.execute_reply":"2021-06-03T20:54:09.753852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_training_set = final_features.values.tolist()\n\nvalidation_set = total_training_set[:num_validation]\ntraining_set = total_training_set[num_validation:]","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:54:09.755879Z","iopub.execute_input":"2021-06-03T20:54:09.756425Z","iopub.status.idle":"2021-06-03T20:54:09.765134Z","shell.execute_reply.started":"2021-06-03T20:54:09.756375Z","shell.execute_reply":"2021-06-03T20:54:09.76429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class Dataset_ECG(torch.utils.data.Dataset):\n#     \"\"\"\n#         Build ECG dataset\n#     \"\"\"\n#     def __init__(self, dataset, num_classes=12):\n#         \"\"\"\n#             dataset을 읽어들여 id, age, sex, recording, labels를 저장한 list를 만들어 줍니다.\n#         \"\"\"\n#         self.sample_id = []\n#         self.sample_recording = []\n#         self.sample_labels = []\n#         self.sample_age = []\n#         self.sample_sex = []\n#         self.sample_f1 = []\n#         self.sample_f2 = []\n#         self.sample_f3 = []\n#         self.sample_f4 = []\n#         self.sample_f5 = []\n#         self.num_samples = len(dataset)\n        \n#         for idx in range(self.num_samples):\n#             _id, _age, _sex, _recording, _labels, _f1, _f2, _f3, _f4, _f5 = dataset[idx]\n#             # model에 input으로 들어가는 data는 torch.Tensor 타입으로 변환해 줍니다.\n#             age = torch.tensor(_age)\n#             sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n#             recording = torch.tensor(_recording)\n#             labels = torch.tensor(np.zeros(num_classes))\n#             for label in _labels:\n#                 labels[int(label)] = 1\n#             f1 = torch.tensor(_f1)\n#             f2 = torch.tensor(_f2)\n#             f3 = torch.tensor(_f3)\n#             f4 = torch.tensor(_f4)\n#             f5 = torch.tensor(_f5)\n\n#             self.sample_id.append(_id)\n#             self.sample_age.append(age)\n#             self.sample_sex.append(sex)\n#             self.sample_recording.append(recording)\n#             self.sample_labels.append(labels)\n#             self.sample_f1.append(f1)\n#             self.sample_f2.append(f2)\n#             self.sample_f3.append(f3)\n#             self.sample_f4.append(f4)\n#             self.sample_f5.append(f5)\n\n#         print(f'Loaded {self.num_samples} samples...')\n\n#     def __len__(self):\n#         return self.num_samples\n\n#     def __getitem__(self, idx):\n#         return {\n#             \"id\": self.sample_id[idx],\n#             \"age\": self.sample_age[idx],\n#             \"sex\": self.sample_sex[idx],\n#             \"recording\": self.sample_recording[idx],\n#             \"labels\": self.sample_labels[idx],\n#             \"etc_features\": torch.cat((self.sample_age[idx].reshape(1),\n#                                        self.sample_sex[idx].reshape(1),\n#                                        self.sample_f1[idx].reshape(1),\n#                                        self.sample_f2[idx].reshape(1),\n#                                        self.sample_f3[idx].reshape(1),\n#                                        self.sample_f4[idx].reshape(1),\n#                                        self.sample_f5[idx].reshape(1)\n\n#                                       )),\n#             \"f1\": self.sample_f1[idx],\n#             \"f2\": self.sample_f2[idx],\n#             \"f3\": self.sample_f3[idx],\n#             \"f4\": self.sample_f4[idx],\n#             \"f5\": self.sample_f5[idx]\n#         }","metadata":{"id":"N1_TG777rz5R","executionInfo":{"status":"ok","timestamp":1622174905772,"user_tz":-540,"elapsed":15,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-03T20:54:09.766459Z","iopub.execute_input":"2021-06-03T20:54:09.766852Z","iopub.status.idle":"2021-06-03T20:54:09.775254Z","shell.execute_reply.started":"2021-06-03T20:54:09.766814Z","shell.execute_reply":"2021-06-03T20:54:09.774397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset_ECG(torch.utils.data.Dataset):\n    \"\"\"\n        Build ECG dataset\n    \"\"\"\n    def __init__(self, dataset, num_classes=12):\n        \"\"\"\n            dataset을 읽어들여 id, age, sex, recording, labels를 저장한 list를 만들어 줍니다.\n        \"\"\"\n        self.sample_id = []\n        self.sample_age = []\n        self.sample_sex = []\n        self.sample_recording = []\n        self.sample_labels = []\n        self.num_samples = len(dataset)\n        \n        for idx in range(self.num_samples):\n            _id, _age, _sex, _recording, _labels = dataset[idx]\n            # model에 input으로 들어가는 data는 torch.Tensor 타입으로 변환해 줍니다.\n            age = torch.tensor(_age)\n            sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n            recording = torch.tensor(_recording)\n            labels = torch.tensor(np.zeros(num_classes))\n            for label in _labels:\n                labels[int(label)] = 1\n\n            self.sample_id.append(_id)\n            self.sample_age.append(age)\n            self.sample_sex.append(sex)\n            self.sample_recording.append(recording)\n            self.sample_labels.append(labels)\n\n        print(f'Loaded {self.num_samples} samples...')\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return {\n            \"id\": self.sample_id[idx],\n            \"age\": self.sample_age[idx],\n            \"sex\": self.sample_sex[idx],\n            \"recording\": self.sample_recording[idx],\n            \"labels\": self.sample_labels[idx]\n        }","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:54:09.777459Z","iopub.execute_input":"2021-06-03T20:54:09.777708Z","iopub.status.idle":"2021-06-03T20:54:09.791152Z","shell.execute_reply.started":"2021-06-03T20:54:09.777684Z","shell.execute_reply":"2021-06-03T20:54:09.790025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nresnet for 1-d signal data, pytorch version\n \nShenda Hong, Oct 2019\n\"\"\"\n#https://github.com/hsd1503/resnet1d/blob/master/resnet1d.py\n\nimport numpy as np\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import classification_report \n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MyDataset(Dataset):\n    def __init__(self, data, label):\n        self.data = data\n        self.label = label\n\n    def __getitem__(self, index):\n        return (torch.tensor(self.data[index], dtype=torch.float), torch.tensor(self.label[index], dtype=torch.long))\n\n    def __len__(self):\n        return len(self.data)\n    \nclass MyConv1dPadSame(nn.Module):\n    \"\"\"\n    extend nn.Conv1d to support SAME padding\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1):\n        super(MyConv1dPadSame, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.groups = groups\n        self.conv = torch.nn.Conv1d(\n            in_channels=self.in_channels, \n            out_channels=self.out_channels, \n            kernel_size=self.kernel_size, \n            stride=self.stride, \n            groups=self.groups)\n\n    def forward(self, x):\n        \n        net = x\n        \n        # compute pad shape\n        in_dim = net.shape[-1]\n        out_dim = (in_dim + self.stride - 1) // self.stride\n        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n        pad_left = p // 2\n        pad_right = p - pad_left\n        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n        \n        net = self.conv(net)\n\n        return net\n\n        \nclass MyMaxPool1dPadSame(nn.Module):\n    \"\"\"\n    extend nn.MaxPool1d to support SAME padding\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(MyMaxPool1dPadSame, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = 1\n        self.max_pool = torch.nn.MaxPool1d(kernel_size=self.kernel_size)\n\n    def forward(self, x):\n        \n        net = x\n        \n        # compute pad shape\n        in_dim = net.shape[-1]\n        out_dim = (in_dim + self.stride - 1) // self.stride\n        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n        pad_left = p // 2\n        pad_right = p - pad_left\n        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n        \n        net = self.max_pool(net)\n        \n        return net\n    \nclass BasicBlock(nn.Module):\n    \"\"\"\n    ResNet Basic Block\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, downsample, use_bn, use_do, is_first_block=False):\n        super(BasicBlock, self).__init__()\n        \n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.out_channels = out_channels\n        self.stride = stride\n        self.groups = groups\n        self.downsample = downsample\n        if self.downsample:\n            self.stride = stride\n        else:\n            self.stride = 1\n        self.is_first_block = is_first_block\n        self.use_bn = use_bn\n        self.use_do = use_do\n\n        # the first conv\n        self.bn1 = nn.BatchNorm1d(in_channels)\n        self.relu1 = nn.ReLU()\n        self.do1 = nn.Dropout(p=0.5)\n        self.conv1 = MyConv1dPadSame(\n            in_channels=in_channels, \n            out_channels=out_channels, \n            kernel_size=kernel_size, \n            stride=self.stride,\n            groups=self.groups)\n\n        # the second conv\n        self.bn2 = nn.BatchNorm1d(out_channels)\n        self.relu2 = nn.ReLU()\n        self.do2 = nn.Dropout(p=0.5)\n        self.conv2 = MyConv1dPadSame(\n            in_channels=out_channels, \n            out_channels=out_channels, \n            kernel_size=kernel_size, \n            stride=1,\n            groups=self.groups)\n                \n        self.max_pool = MyMaxPool1dPadSame(kernel_size=self.stride)\n\n    def forward(self, x):\n        \n        identity = x\n        \n        # the first conv\n        out = x\n        if not self.is_first_block:\n            if self.use_bn:\n                out = self.bn1(out)\n            out = self.relu1(out)\n            if self.use_do:\n                out = self.do1(out)\n        out = self.conv1(out)\n        \n        # the second conv\n        if self.use_bn:\n            out = self.bn2(out)\n        out = self.relu2(out)\n        if self.use_do:\n            out = self.do2(out)\n        out = self.conv2(out)\n        \n        # if downsample, also downsample identity\n        if self.downsample:\n            identity = self.max_pool(identity)\n            \n        # if expand channel, also pad zeros to identity\n        if self.out_channels != self.in_channels:\n            identity = identity.transpose(-1,-2)\n            ch1 = (self.out_channels-self.in_channels)//2\n            ch2 = self.out_channels-self.in_channels-ch1\n            identity = F.pad(identity, (ch1, ch2), \"constant\", 0)\n            identity = identity.transpose(-1,-2)\n        \n        # shortcut\n        out += identity\n\n        return out\n    \nclass ResNet1D(nn.Module):\n    \"\"\"\n    \n    Input:\n        X: (n_samples, n_channel, n_length)\n        Y: (n_samples)\n        \n    Output:\n        out: (n_samples)\n        \n    Pararmetes:\n        in_channels: dim of input, the same as n_channel\n        base_filters: number of filters in the first several Conv layer, it will double at every 4 layers\n        kernel_size: width of kernel\n        stride: stride of kernel moving\n        groups: set larget to 1 as ResNeXt\n        n_block: number of blocks\n        n_classes: number of classes\n        \n    \"\"\"\n\n    def __init__(self, in_channels, base_filters, kernel_size, stride, groups, n_block, n_classes, downsample_gap=2, increasefilter_gap=4, use_bn=True, use_do=True, verbose=False):\n        super(ResNet1D, self).__init__()\n        \n        self.verbose = verbose\n        self.n_block = n_block\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.groups = groups\n        self.use_bn = use_bn\n        self.use_do = use_do\n\n        self.downsample_gap = downsample_gap # 2 for base model\n        self.increasefilter_gap = increasefilter_gap # 4 for base model\n\n        # first block\n        self.first_block_conv = MyConv1dPadSame(in_channels=in_channels, out_channels=base_filters, kernel_size=self.kernel_size, stride=1)\n        self.first_block_bn = nn.BatchNorm1d(base_filters)\n        self.first_block_relu = nn.ReLU()\n        out_channels = base_filters\n                \n        # residual blocks\n        self.basicblock_list = nn.ModuleList()\n        for i_block in range(self.n_block):\n            # is_first_block\n            if i_block == 0:\n                is_first_block = True\n            else:\n                is_first_block = False\n            # downsample at every self.downsample_gap blocks\n            if i_block % self.downsample_gap == 1:\n                downsample = True\n            else:\n                downsample = False\n            # in_channels and out_channels\n            if is_first_block:\n                in_channels = base_filters\n                out_channels = in_channels\n            else:\n                # increase filters at every self.increasefilter_gap blocks\n                in_channels = int(base_filters*2**((i_block-1)//self.increasefilter_gap))\n                if (i_block % self.increasefilter_gap == 0) and (i_block != 0):\n                    out_channels = in_channels * 2\n                else:\n                    out_channels = in_channels\n            \n            tmp_block = BasicBlock(\n                in_channels=in_channels, \n                out_channels=out_channels, \n                kernel_size=self.kernel_size, \n                stride = self.stride, \n                groups = self.groups, \n                downsample=downsample, \n                use_bn = self.use_bn, \n                use_do = self.use_do, \n                is_first_block=is_first_block)\n            self.basicblock_list.append(tmp_block)\n\n        # final prediction\n        self.final_bn = nn.BatchNorm1d(out_channels)\n        self.final_relu = nn.ReLU(inplace=True)\n        # self.do = nn.Dropout(p=0.5)\n        self.dense = nn.Linear(out_channels, n_classes)\n        # self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        \n        out = x\n        \n        # first conv\n        if self.verbose:\n            print('input shape', out.shape)\n        out = self.first_block_conv(out)\n        if self.verbose:\n            print('after first conv', out.shape)\n        if self.use_bn:\n            out = self.first_block_bn(out)\n        out = self.first_block_relu(out)\n        \n        # residual blocks, every block has two conv\n        for i_block in range(self.n_block):\n            net = self.basicblock_list[i_block]\n            if self.verbose:\n                print('i_block: {0}, in_channels: {1}, out_channels: {2}, downsample: {3}'.format(i_block, net.in_channels, net.out_channels, net.downsample))\n            out = net(out)\n            if self.verbose:\n                print(out.shape)\n\n        # final prediction\n        if self.use_bn:\n            out = self.final_bn(out)\n        out = self.final_relu(out)\n        out = out.mean(-1)\n        if self.verbose:\n            print('final pooling', out.shape)\n        # out = self.do(out)\n        out = self.dense(out)\n        if self.verbose:\n            print('dense', out.shape)\n        # out = self.softmax(out)\n        if self.verbose:\n            print('softmax', out.shape)\n        \n        return out    ","metadata":{"id":"SL8tAhIar1GK","executionInfo":{"status":"ok","timestamp":1622174906912,"user_tz":-540,"elapsed":1154,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-03T20:54:09.793305Z","iopub.execute_input":"2021-06-03T20:54:09.793678Z","iopub.status.idle":"2021-06-03T20:54:10.121435Z","shell.execute_reply.started":"2021-06-03T20:54:09.793642Z","shell.execute_reply":"2021-06-03T20:54:10.120572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \"\"\"\n    \n    Input:\n        X: (n_samples, n_channel, n_length)\n        Y: (n_samples)\n        \n    Output:\n        out: (n_samples)\n        \n    Pararmetes:\n        in_channels: dim of input, the same as n_channel\n        base_filters: number of filters in the first several Conv layer, it will double at every 4 layers\n        kernel_size: width of kernel\n        stride: stride of kernel moving\n        groups: set larget to 1 as ResNeXt\n        n_block: number of blocks\n        n_classes: number of classes\n        \n    \"\"\"\n\nEPOCHS = 30\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001\n\nkernel_size = 16\nstride = 2\nn_block = 48\ndownsample_gap = 6\nincreasefilter_gap = 12\n\nmodel = ResNet1D(\n    in_channels=2, \n    base_filters=128, # 64 for ResNet1D, 352 for ResNeXt1D\n    kernel_size=kernel_size, \n    stride=stride, \n    groups=32, \n    n_block=n_block, \n    n_classes=12, \n    downsample_gap=downsample_gap, \n    increasefilter_gap=increasefilter_gap, \n    use_do=True)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"Dm61ynFRr4aQ","executionInfo":{"status":"ok","timestamp":1622174906916,"user_tz":-540,"elapsed":7,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-03T20:54:10.122687Z","iopub.execute_input":"2021-06-03T20:54:10.123065Z","iopub.status.idle":"2021-06-03T20:54:10.314751Z","shell.execute_reply.started":"2021-06-03T20:54:10.123028Z","shell.execute_reply":"2021-06-03T20:54:10.313895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 위에서 정의한 Dataset_ECG를 활용해 training dataset을 만들어 줍니다.\ntraining_dataset = Dataset_ECG(training_set, num_classes=12)\n# Training dataset을 batch 단위로 읽어들일 수 있도록 DataLoader를 만들어줍니다.\ntraining_loader = torch.utils.data.DataLoader(training_dataset, pin_memory=True, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:54:10.316087Z","iopub.execute_input":"2021-06-03T20:54:10.316455Z","iopub.status.idle":"2021-06-03T20:54:11.407344Z","shell.execute_reply.started":"2021-06-03T20:54:10.31642Z","shell.execute_reply":"2021-06-03T20:54:11.4063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nmodel.train()\n\ncriterion = torch.nn.BCEWithLogitsLoss() # for multi-label classification\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:54:11.408768Z","iopub.execute_input":"2021-06-03T20:54:11.409175Z","iopub.status.idle":"2021-06-03T20:54:13.269489Z","shell.execute_reply.started":"2021-06-03T20:54:11.409135Z","shell.execute_reply":"2021-06-03T20:54:13.268637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(1, 2):\n    print(f'***** Epoch {epoch} *****')\n    epoch_training_loss_sum = 0.0\n    for i_batch, sample_batched in enumerate(training_loader):\n        b_recording = sample_batched[\"recording\"].to(device)\n        b_labels = sample_batched[\"labels\"].to(device)\n        optimizer.zero_grad()\n        b_out = model(b_recording)\n        loss = criterion(b_out, b_labels)\n        loss.backward()\n        optimizer.step()\n        epoch_training_loss_sum += loss.item() * b_labels.shape[0]\n\n    epoch_training_loss = epoch_training_loss_sum / num_training\n    print(f'training loss of epoch {epoch}: {epoch_training_loss}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:17:37.116662Z","iopub.execute_input":"2021-06-04T03:17:37.117057Z","iopub.status.idle":"2021-06-04T03:31:11.323026Z","shell.execute_reply.started":"2021-06-04T03:17:37.117023Z","shell.execute_reply":"2021-06-04T03:31:11.322021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Training loop\n# for epoch in range(1, EPOCHS+1):\n#     LEARNING_RATE=0.0001\n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n#     print(f'***** Epoch {epoch} *****')\n#     epoch_training_loss_sum = 0.0\n#     for i_batch, sample_batched in enumerate(training_loader):\n#         b_recording = sample_batched[\"recording\"].to(device)\n#         b_etc_features = sample_batched[\"etc_features\"].to(device)\n#         b_labels = sample_batched[\"labels\"].to(device)\n#         optimizer.zero_grad()\n#         b_out = model(b_recording, b_etc_features)\n#         loss = criterion(b_out, b_labels)\n#         loss.backward()\n#         optimizer.step()\n#         epoch_training_loss_sum += loss.item() * b_labels.shape[0]\n\n#     epoch_training_loss = epoch_training_loss_sum / num_training\n#     print(f'training loss of epoch {epoch}: {epoch_training_loss}\\n')   \n#     model.eval()\n#     if epoch > 25:\n#         validation_prediction_df = pd.DataFrame(columns=['labels'])\n#         validation_prediction_df.index.name = 'id'\n#         validation_true_labels_df = pd.DataFrame(columns=['labels'])\n#         validation_true_labels_df.index.name = 'id'\n\n#         with torch.no_grad():\n#             for idx in range(len(validation_set)):\n#                 validation_sample = validation_set[idx]\n#                 _, _, _, recording, labels = validation_sample\n#                 out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n#                 sample_prediction = torch.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n#                 indices_of_1s = np.where(sample_prediction.cpu())[0]\n#                 str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n#                 validation_prediction_df.loc[idx] = [str_indices_of_1s]\n        \n#                 str_true_labels = ' '.join(labels)\n#                 validation_true_labels_df.loc[idx] = [str_true_labels]\n#         from sklearn.preprocessing import MultiLabelBinarizer\n#         from sklearn.metrics import f1_score\n\n#         mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n#         mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n\n#         macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n#         print(f'macro f1 score on validation set: {macro_f1_validation}')\n\n#         s = '/content/drive/MyDrive/heartprediction/resnet_last1_'\n#         y = str(epoch)\n#         name = s + y\n#         name = name +'.pt'\n#         torch.save(model,name)\n#     print(time.time()-t0)","metadata":{"id":"Q883BrDTr7cK","executionInfo":{"status":"error","timestamp":1622183227129,"user_tz":-540,"elapsed":1134115,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"outputId":"62df3e0e-eb5f-4657-eeaf-66c7ef3b765f","execution":{"iopub.status.busy":"2021-06-04T02:31:09.974921Z","iopub.status.idle":"2021-06-04T02:31:09.975328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nvalidation_prediction_df = pd.DataFrame(columns=['labels'])\nvalidation_prediction_df.index.name = 'id'\nvalidation_true_labels_df = pd.DataFrame(columns=['labels'])\nvalidation_true_labels_df.index.name = 'id'\n\nwith torch.no_grad():\n    for idx in range(len(validation_set)):\n        validation_sample = validation_set[idx]\n        _, _, _, recording, labels = validation_sample\n        out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n        sample_prediction = torch.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n        indices_of_1s = np.where(sample_prediction.cpu())[0]\n        str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n        validation_prediction_df.loc[idx] = [str_indices_of_1s]\n        \n        str_true_labels = ' '.join(labels)\n        validation_true_labels_df.loc[idx] = [str_true_labels]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:33:31.184644Z","iopub.execute_input":"2021-06-04T03:33:31.185018Z","iopub.status.idle":"2021-06-04T03:38:13.537772Z","shell.execute_reply.started":"2021-06-04T03:33:31.184986Z","shell.execute_reply":"2021-06-04T03:38:13.536948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.eval()\n\n# validation_prediction_df = pd.DataFrame(columns=['labels'])\n# validation_prediction_df.index.name = 'id'\n# validation_true_labels_df = pd.DataFrame(columns=['labels'])\n# validation_true_labels_df.index.name = 'id'\n\n# with torch.no_grad():\n#     for idx in range(len(validation_set)):\n#         validation_sample = validation_set[idx]\n#         _id, _age, _sex, recording, labels, _f1, _f2, _f3, _f4, _f5= validation_sample\n#         _sex = 0 if _sex == \"F\" else 1\n#         etc_features = torch.tensor([_age, _sex, _f1, _f2, _f3, _f4, _f5])\n#         #etc_features = torch.tensor([_age, _sex])\n#         out = model(torch.tensor(recording).unsqueeze(0).to(device), etc_features.unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n#         sample_prediction = torch.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n#         indices_of_1s = np.where(sample_prediction.cpu())[0]\n#         str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n#         validation_prediction_df.loc[idx] = [str_indices_of_1s]\n        \n#         str_true_labels = ' '.join(labels)\n#         validation_true_labels_df.loc[idx] = [str_true_labels]","metadata":{"id":"GYwP5loOG9PZ","executionInfo":{"status":"aborted","timestamp":1622183227132,"user_tz":-540,"elapsed":3,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-04T02:31:09.977824Z","iopub.status.idle":"2021-06-04T02:31:09.978379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import f1_score\n\nmlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\nmlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n\nmacro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\nprint(f'macro f1 score on validation set: {macro_f1_validation}')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:38:13.539202Z","iopub.execute_input":"2021-06-04T03:38:13.539587Z","iopub.status.idle":"2021-06-04T03:38:13.567419Z","shell.execute_reply.started":"2021-06-04T03:38:13.539547Z","shell.execute_reply":"2021-06-04T03:38:13.56662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_set = sorted(read_files(test_dir, is_training=False), key=lambda sample:sample[0])\n\n# new_features = pd.read_csv('../input/feature-extraction-3/feature_extraction_test_data1.csv')\n# new_features.drop(['Unnamed: 0', 'id'], axis=1, inplace=True)\n\n# original_features = pd.DataFrame(test_set)\n# original_features.columns = ['id', 'age', 'sex', 'recording']\n\n# final_features = pd.concat([original_features, new_features], axis=1)\n# final_features = final_features.fillna(0)\n# final_features.to_csv('test_set.csv')\n# test_set = final_features.values.tolist()\n\n# num_test = len(test_set)\n# print(f'Number of test samples: {num_test}')","metadata":{"id":"8EM5HDItHJFy","executionInfo":{"status":"aborted","timestamp":1622183227132,"user_tz":-540,"elapsed":3,"user":{"displayName":"­김연수 / 학생 / 자유전공학부","photoUrl":"","userId":"16035193206220113345"}},"execution":{"iopub.status.busy":"2021-06-04T02:31:09.981389Z","iopub.status.idle":"2021-06-04T02:31:09.981946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = sorted(read_files(test_dir, is_training=False), key=lambda sample:sample[0])\nnum_test = len(test_set)\nprint(f'Number of test samples: {num_test}')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:38:13.570126Z","iopub.execute_input":"2021-06-04T03:38:13.570378Z","iopub.status.idle":"2021-06-04T03:38:26.604307Z","shell.execute_reply.started":"2021-06-04T03:38:13.570352Z","shell.execute_reply":"2021-06-04T03:38:26.603392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ntest_prediction_df = pd.DataFrame(columns=['labels'])\ntest_prediction_df.index.name = 'id'\n\nwith torch.no_grad():\n    for idx in range(len(test_set)):\n        test_sample = test_set[idx]\n        _, _, _, recording = test_sample\n        out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n        sample_prediction = torch.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n        indices_of_1s = np.where(sample_prediction.cpu())[0]\n        str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n        test_prediction_df.loc[idx] = [str_indices_of_1s]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:38:26.605715Z","iopub.execute_input":"2021-06-04T03:38:26.606212Z","iopub.status.idle":"2021-06-04T03:47:15.74236Z","shell.execute_reply.started":"2021-06-04T03:38:26.606172Z","shell.execute_reply":"2021-06-04T03:47:15.741484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prediction_df.to_csv('submission3.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:49:16.101463Z","iopub.execute_input":"2021-06-04T03:49:16.101784Z","iopub.status.idle":"2021-06-04T03:49:16.123276Z","shell.execute_reply.started":"2021-06-04T03:49:16.101754Z","shell.execute_reply":"2021-06-04T03:49:16.122493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_test_prediction_df = test_prediction_df.replace(\"\", \"8\")","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:48:11.284022Z","iopub.execute_input":"2021-06-04T03:48:11.284355Z","iopub.status.idle":"2021-06-04T03:48:11.28941Z","shell.execute_reply.started":"2021-06-04T03:48:11.284325Z","shell.execute_reply":"2021-06-04T03:48:11.28824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_test_prediction_df.to_csv(\"submission_imputed3.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:48:39.08428Z","iopub.execute_input":"2021-06-04T03:48:39.084641Z","iopub.status.idle":"2021-06-04T03:48:39.107339Z","shell.execute_reply.started":"2021-06-04T03:48:39.084609Z","shell.execute_reply":"2021-06-04T03:48:39.106559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.eval()\n\n# test_prediction_df = pd.DataFrame(columns=['labels'])\n# test_prediction_df.index.name = 'id'\n\n# with torch.no_grad():\n#     for idx in range(len(test_set)):\n#         test_sample = test_set[idx]\n#         _id, _age, _sex, recording, _f1, _f2, _f3, _f4, _f5 = test_sample\n#         _sex = 0 if _sex == \"F\" else 1\n#         etc_features = torch.tensor([_age, _sex, _f1, _f2, _f3, _f4, _f5])\n#         #etc_features = torch.tensor([_age, _sex])\n#         out = model(torch.tensor(recording).unsqueeze(0).to(device), etc_features.unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n#         sample_prediction = torch.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n#         indices_of_1s = np.where(sample_prediction.cpu())[0]\n#         str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n#         test_prediction_df.loc[idx] = [str_indices_of_1s]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T02:31:09.986528Z","iopub.status.idle":"2021-06-04T02:31:09.987097Z"},"trusted":true},"execution_count":null,"outputs":[]}]}